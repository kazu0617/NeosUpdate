2021.3.12.44
Hello everyone! With the release of the Vive Facial Tracker, here's a bunch more eye and lip/mouth tracking improvements, to make the setup of avatars easier and more automated.

Heuristics of the blendshape matching were improved significantly and there's now a checkbox in the Avatar Creator to automatically set it up for face tracking. It's off by default, as it can mess up avatars if they're not setup for it, but feel free to experiment and see how well it works. You'll get best results following HTC's naming scheme (tutorial & samples coming soon).

There are some important improvements for the new desktop mode as well, notably with hand posing, which is now exact and capable of aiming the laser perfectly straight at the target. This is an improtant pre-requisite for implementing tools, as it will allow the system to aim the tool exactly at the point where the screen cursor is pointing.

A bunch of other improvements, tweaks and additions too, the VIVE Hand Tracking SDK was updated too (they seem to have made quite good improvements on the tracking quality!) and a few new blendshapes for the eye tracking that Neos didn't register before.

More to come soon!

New features:
- Added "Setup Face Tracking" to the avatar creator, which will scan the blendshapes on the avatar and attempt to setup face tracking
-- This can be used to easily setup avatars for the Vive Facial Tracker
-- Note that success rate will depend on the available blendshapes, Neos will use name heuristics to try to find best matches. You can get best results by following the naming convention from the HTC sample model

- Added Squeeze and Frown eye tracking parameters (based on feedback from @Reactant)
-- Appropriate fields were added to EyeManager and EyeLinearDriver
-- Squeeze indicates how tightly is the eye closed
-- Frown indicates the eye frowning (note that from my testing this doesn't seem to be currently tracked with Vive Pro Eye)
- Added StrengthMultiplier to AvatarExpressionDriver, which allows applying a global scale to the driven targets/blendsdhapes
- Added "Show Laser in Desktop Mode" setting which will display the laser in the new desktop mode when pointing at interactable objects
-- I mostly use this for debugging, but you can use this if you prefer for immersion

Tweaks:
- Heavily improved hand posing in desktop mode when grabbing items and interacting with them
-- The actual position/direction of the laser is now respected, posing the hand exactly so the laser is straight when at rest and hand is properly offset for different avatars
--- This is an important mechanism for tool support, as it allows to pose the hand to aim at exactly specific point in the world
-- When the target point is near face, the hand is pushed to the side and down to avoid it from intersecting/obscuring the face/viewpoint
-- The hand interaction transition now tracks velocity, preventing instant snapping of movement direction and has tweaked velocities/smoothing
- The cursor reticle now fades away after 2 seconds of inactivity in the desktop mode

- Sliding items all the way towards the face will no longer equip/physically grab in then desktop mode
- Improved internal moderation tools to allow for more finegrained control (based on feedback by the moderation team)
- Userspace laser in desktop mode now correctly overrides the world-space laser, rather than having both activate at the same time
- Holding items in desktop mode no longer uses the simulated hand's twist to rotate the item, to prevent unwanted rotations when moving the item around
-- Note that some partial rotation might still be transferred when initially grabbing
- Improved Avatar Expression driver blendshape assignment heuristics
-- The heuristics now detects more keywords, improving face tracking support for Ready Player Me models and others
-- The assignment system now also has a face target filtering, ignoring ambigous blendshapes if non-ambigious are present (e.g. "Mouth Smile" vs just "Smile") and preventing multiple similar blendshapes from being all assigned, causing over-driving (e.g. having both "Mouth Open" and "Jaw Open")
- EyeManager parameters now use sliders to represent values within the 0...1 range
- Increased Stiffness on DynamicBoneChain decimal points from 2 to 4 (based on feedback by @H3BO3)

- Updated VIVE Hand Tracking SDK to 0.10.0 (from 0.9.3)
- Updated VIVE SR Anipal SDK (for eye and face tracking) to 1.3.2.0 (from 1.3.1.1)

- Merged Czech locale additions and tweaks by @rampa_3 (UTC +1, DST UTC +2)
- Merged Korean locale tweaks by @MirPASEC
Bugfixes:
- Fixed Interaction Laser raycast portals not working properly when the laser goes through another object that's ignored, causing UI to be unusable
-- This fixes cases of UI not being interactable in desktop mode when using portal raycast and the initial interaction hit hitting the user's avatar head
-- This also fixes cases of UI not being interactable when laser passes through canvas that let's the hit pass through if it doesn't hit anything (e.g. dash not being usable in VR when laser passes through the notifications canvas)
- Fixed grabbing a VRIKAvatar causing the neck and hips position to be taken from the held avatar, even though it's not equipped
- Fixed Cameras rendering the overlay layer
-- This should fix the mirror facet from rendering the dash while the desktop mode is enabled (re-reported by @epicEaston197 and @Khosumi)
- Fixed DictionaryList getting into invalid state when an operation during enumeration results in a single remaining item
-- This fixes various random bugs and potential corruptions due to using a list that's been returned back to memory pool
- Fixed out of range inputs for the maxUsers command on headless throwing an exception (reported by @Glitch)
- Fixed RecordSyncStatus not reporting sync error when the error is anything other than Out Of Space (based on report by @PlasmaRaven, @Elektrospy, @Honeypotbutt and @Shifty | Quality Control Lead)